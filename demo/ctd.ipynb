{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>project</th>\n",
       "      <th>date</th>\n",
       "      <th>sampling_bout</th>\n",
       "      <th>gear</th>\n",
       "      <th>sample_grp</th>\n",
       "      <th>station</th>\n",
       "      <th>location</th>\n",
       "      <th>replicate</th>\n",
       "      <th>primers</th>\n",
       "      <th>...</th>\n",
       "      <th>White_catfish</th>\n",
       "      <th>Atl_salmon</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>surface_temp</th>\n",
       "      <th>surface_salt</th>\n",
       "      <th>surface_pH</th>\n",
       "      <th>bottom_temp</th>\n",
       "      <th>bottom_salt</th>\n",
       "      <th>bottom_pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NJOT_Feb_24_3</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.182897</td>\n",
       "      <td>-74.204214</td>\n",
       "      <td>7.207000</td>\n",
       "      <td>19.924286</td>\n",
       "      <td>7.894286</td>\n",
       "      <td>6.984000</td>\n",
       "      <td>22.357692</td>\n",
       "      <td>7.856154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NJOT_Feb_24_5</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.104352</td>\n",
       "      <td>-74.207144</td>\n",
       "      <td>7.572200</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>7.864000</td>\n",
       "      <td>7.327800</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>7.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJOT_Feb_24_6</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>6</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.272691</td>\n",
       "      <td>-74.092105</td>\n",
       "      <td>7.175250</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>7.983750</td>\n",
       "      <td>7.034833</td>\n",
       "      <td>22.330000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NJOT_Feb_24_7</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>7</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.325567</td>\n",
       "      <td>-74.050293</td>\n",
       "      <td>7.141714</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>7.902857</td>\n",
       "      <td>6.951571</td>\n",
       "      <td>0.066429</td>\n",
       "      <td>7.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NJOT_Feb_24_8</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>8</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.215071</td>\n",
       "      <td>-74.129712</td>\n",
       "      <td>7.747545</td>\n",
       "      <td>21.470000</td>\n",
       "      <td>7.875455</td>\n",
       "      <td>7.162100</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NJOT_Feb_24_10</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.217271</td>\n",
       "      <td>-74.052457</td>\n",
       "      <td>7.412000</td>\n",
       "      <td>22.007000</td>\n",
       "      <td>7.865000</td>\n",
       "      <td>7.383020</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>7.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NJOT_Feb_24_11</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>11</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.216981</td>\n",
       "      <td>-74.440575</td>\n",
       "      <td>5.550700</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>7.836000</td>\n",
       "      <td>5.717857</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>7.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NJOT_Feb_24_14</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>14</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.403917</td>\n",
       "      <td>-74.286663</td>\n",
       "      <td>4.890875</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.832500</td>\n",
       "      <td>5.516107</td>\n",
       "      <td>20.027500</td>\n",
       "      <td>7.783571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NJOT_Feb_24_15</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.550527</td>\n",
       "      <td>-74.130042</td>\n",
       "      <td>6.065100</td>\n",
       "      <td>18.135000</td>\n",
       "      <td>7.849000</td>\n",
       "      <td>6.225100</td>\n",
       "      <td>21.723000</td>\n",
       "      <td>7.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NJOT_Feb_24_16</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>16</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.472763</td>\n",
       "      <td>-74.004055</td>\n",
       "      <td>6.678375</td>\n",
       "      <td>22.031250</td>\n",
       "      <td>7.835000</td>\n",
       "      <td>6.654333</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>7.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NJOT_Feb_24_20</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>20</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.068439</td>\n",
       "      <td>-74.127731</td>\n",
       "      <td>7.123857</td>\n",
       "      <td>22.711429</td>\n",
       "      <td>7.862857</td>\n",
       "      <td>7.379176</td>\n",
       "      <td>15.897059</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NJOT_Feb_24_21</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>21</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.991007</td>\n",
       "      <td>-73.972367</td>\n",
       "      <td>7.699333</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>7.851667</td>\n",
       "      <td>7.938500</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NJOT_Feb_24_22</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>22</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.103433</td>\n",
       "      <td>-73.964745</td>\n",
       "      <td>7.214500</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>7.888333</td>\n",
       "      <td>7.776800</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NJOT_Feb_24_24</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>24</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.364163</td>\n",
       "      <td>-73.896414</td>\n",
       "      <td>7.955429</td>\n",
       "      <td>20.768571</td>\n",
       "      <td>7.914286</td>\n",
       "      <td>7.200857</td>\n",
       "      <td>20.779286</td>\n",
       "      <td>7.849286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NJOT_Feb_24_25</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>25</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.288600</td>\n",
       "      <td>-73.739525</td>\n",
       "      <td>7.321667</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>7.858333</td>\n",
       "      <td>7.589667</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>7.824167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NJOT_Feb_24_26</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>26</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.476666</td>\n",
       "      <td>-73.891367</td>\n",
       "      <td>6.898286</td>\n",
       "      <td>21.882857</td>\n",
       "      <td>7.858571</td>\n",
       "      <td>7.142786</td>\n",
       "      <td>22.370000</td>\n",
       "      <td>7.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NJOT_Feb_24_27</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>27</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.472270</td>\n",
       "      <td>-73.740292</td>\n",
       "      <td>6.343000</td>\n",
       "      <td>11.487500</td>\n",
       "      <td>7.872500</td>\n",
       "      <td>7.461889</td>\n",
       "      <td>22.536667</td>\n",
       "      <td>7.853333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NJOT_Feb_24_28</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>28</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.734484</td>\n",
       "      <td>-73.972144</td>\n",
       "      <td>6.542500</td>\n",
       "      <td>19.558750</td>\n",
       "      <td>7.863750</td>\n",
       "      <td>6.364842</td>\n",
       "      <td>21.901579</td>\n",
       "      <td>7.845789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NJOT_Feb_24_29</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>29</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.660862</td>\n",
       "      <td>-73.896875</td>\n",
       "      <td>7.032000</td>\n",
       "      <td>21.691667</td>\n",
       "      <td>7.868333</td>\n",
       "      <td>6.526176</td>\n",
       "      <td>22.053529</td>\n",
       "      <td>7.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NJOT_Feb_24_30</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>30</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.663942</td>\n",
       "      <td>-73.740970</td>\n",
       "      <td>7.131667</td>\n",
       "      <td>17.976667</td>\n",
       "      <td>7.870000</td>\n",
       "      <td>6.400722</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NJOT_Feb_24_31</td>\n",
       "      <td>NJOT</td>\n",
       "      <td>2/1/24</td>\n",
       "      <td>Feb_24</td>\n",
       "      <td>edna</td>\n",
       "      <td>Trawl</td>\n",
       "      <td>31</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>bony</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.582736</td>\n",
       "      <td>-73.973833</td>\n",
       "      <td>6.693222</td>\n",
       "      <td>21.846667</td>\n",
       "      <td>7.864444</td>\n",
       "      <td>6.477400</td>\n",
       "      <td>22.030000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_id project    date sampling_bout  gear sample_grp  station  \\\n",
       "0    NJOT_Feb_24_3    NJOT  2/1/24        Feb_24  edna      Trawl        3   \n",
       "1    NJOT_Feb_24_5    NJOT  2/1/24        Feb_24  edna      Trawl        5   \n",
       "2    NJOT_Feb_24_6    NJOT  2/1/24        Feb_24  edna      Trawl        6   \n",
       "3    NJOT_Feb_24_7    NJOT  2/1/24        Feb_24  edna      Trawl        7   \n",
       "4    NJOT_Feb_24_8    NJOT  2/1/24        Feb_24  edna      Trawl        8   \n",
       "5   NJOT_Feb_24_10    NJOT  2/1/24        Feb_24  edna      Trawl       10   \n",
       "6   NJOT_Feb_24_11    NJOT  2/1/24        Feb_24  edna      Trawl       11   \n",
       "7   NJOT_Feb_24_14    NJOT  2/1/24        Feb_24  edna      Trawl       14   \n",
       "8   NJOT_Feb_24_15    NJOT  2/1/24        Feb_24  edna      Trawl       15   \n",
       "9   NJOT_Feb_24_16    NJOT  2/1/24        Feb_24  edna      Trawl       16   \n",
       "10  NJOT_Feb_24_20    NJOT  2/1/24        Feb_24  edna      Trawl       20   \n",
       "11  NJOT_Feb_24_21    NJOT  2/1/24        Feb_24  edna      Trawl       21   \n",
       "12  NJOT_Feb_24_22    NJOT  2/1/24        Feb_24  edna      Trawl       22   \n",
       "13  NJOT_Feb_24_24    NJOT  2/1/24        Feb_24  edna      Trawl       24   \n",
       "14  NJOT_Feb_24_25    NJOT  2/1/24        Feb_24  edna      Trawl       25   \n",
       "15  NJOT_Feb_24_26    NJOT  2/1/24        Feb_24  edna      Trawl       26   \n",
       "16  NJOT_Feb_24_27    NJOT  2/1/24        Feb_24  edna      Trawl       27   \n",
       "17  NJOT_Feb_24_28    NJOT  2/1/24        Feb_24  edna      Trawl       28   \n",
       "18  NJOT_Feb_24_29    NJOT  2/1/24        Feb_24  edna      Trawl       29   \n",
       "19  NJOT_Feb_24_30    NJOT  2/1/24        Feb_24  edna      Trawl       30   \n",
       "20  NJOT_Feb_24_31    NJOT  2/1/24        Feb_24  edna      Trawl       31   \n",
       "\n",
       "   location  replicate primers  ...  White_catfish  Atl_salmon   latitude  \\\n",
       "0         B          1    bony  ...            0.0         0.0  39.182897   \n",
       "1         B          1    bony  ...            0.0         0.0  39.104352   \n",
       "2         B          1    bony  ...            0.0         0.0  39.272691   \n",
       "3         B          1    bony  ...            0.0         0.0  39.325567   \n",
       "4         B          1    bony  ...            0.0         0.0  39.215071   \n",
       "5         B          1    bony  ...            0.0         0.0  39.217271   \n",
       "6         B          1    bony  ...            0.0         0.0  39.216981   \n",
       "7         B          1    bony  ...            0.0         0.0  39.403917   \n",
       "8         B          1    bony  ...            0.0         0.0  39.550527   \n",
       "9         B          1    bony  ...            0.0         0.0  39.472763   \n",
       "10        B          1    bony  ...            0.0         0.0  39.068439   \n",
       "11        B          1    bony  ...            0.0         0.0  38.991007   \n",
       "12        B          1    bony  ...            0.0         0.0  39.103433   \n",
       "13        B          1    bony  ...            0.0         0.0  39.364163   \n",
       "14        B          1    bony  ...            0.0         0.0  39.288600   \n",
       "15        B          1    bony  ...            0.0         0.0  39.476666   \n",
       "16        B          1    bony  ...            0.0         0.0  39.472270   \n",
       "17        B          1    bony  ...            0.0         0.0  39.734484   \n",
       "18        B          1    bony  ...            0.0         0.0  39.660862   \n",
       "19        B          1    bony  ...            0.0         0.0  39.663942   \n",
       "20        B          1    bony  ...            0.0         0.0  39.582736   \n",
       "\n",
       "    longitude  surface_temp  surface_salt  surface_pH  bottom_temp  \\\n",
       "0  -74.204214      7.207000     19.924286    7.894286     6.984000   \n",
       "1  -74.207144      7.572200      0.060000    7.864000     7.327800   \n",
       "2  -74.092105      7.175250     18.300000    7.983750     7.034833   \n",
       "3  -74.050293      7.141714      0.008571    7.902857     6.951571   \n",
       "4  -74.129712      7.747545     21.470000    7.875455     7.162100   \n",
       "5  -74.052457      7.412000     22.007000    7.865000     7.383020   \n",
       "6  -74.440575      5.550700      0.083000    7.836000     5.717857   \n",
       "7  -74.286663      4.890875     19.000000    7.832500     5.516107   \n",
       "8  -74.130042      6.065100     18.135000    7.849000     6.225100   \n",
       "9  -74.004055      6.678375     22.031250    7.835000     6.654333   \n",
       "10 -74.127731      7.123857     22.711429    7.862857     7.379176   \n",
       "11 -73.972367      7.699333      0.051667    7.851667     7.938500   \n",
       "12 -73.964745      7.214500      0.028333    7.888333     7.776800   \n",
       "13 -73.896414      7.955429     20.768571    7.914286     7.200857   \n",
       "14 -73.739525      7.321667      0.038333    7.858333     7.589667   \n",
       "15 -73.891367      6.898286     21.882857    7.858571     7.142786   \n",
       "16 -73.740292      6.343000     11.487500    7.872500     7.461889   \n",
       "17 -73.972144      6.542500     19.558750    7.863750     6.364842   \n",
       "18 -73.896875      7.032000     21.691667    7.868333     6.526176   \n",
       "19 -73.740970      7.131667     17.976667    7.870000     6.400722   \n",
       "20 -73.973833      6.693222     21.846667    7.864444     6.477400   \n",
       "\n",
       "    bottom_salt  bottom_pH  \n",
       "0     22.357692   7.856154  \n",
       "1      0.037500   7.852500  \n",
       "2     22.330000   7.850000  \n",
       "3      0.066429   7.870000  \n",
       "4     22.380000   7.850000  \n",
       "5     22.400000   7.860000  \n",
       "6      0.105000   7.830000  \n",
       "7     20.027500   7.783571  \n",
       "8     21.723000   7.825000  \n",
       "9     22.150000   7.840000  \n",
       "10    15.897059   7.850000  \n",
       "11     0.028000   7.850000  \n",
       "12     0.041000   7.850000  \n",
       "13    20.779286   7.849286  \n",
       "14     0.087500   7.824167  \n",
       "15    22.370000   7.830000  \n",
       "16    22.536667   7.853333  \n",
       "17    21.901579   7.845789  \n",
       "18    22.053529   7.852941  \n",
       "19    22.000000   7.860000  \n",
       "20    22.030000   7.850000  \n",
       "\n",
       "[21 rows x 94 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('/Users/henrysun_1/Desktop/Duke/2024-2025/Summer 2024/fishics/data/ctd/2024-02_data_all_exo.csv')\n",
    "df = df[df['station'] < 32]\n",
    "\n",
    "data = pd.read_csv(\"/Users/henrysun_1/Desktop/Duke/2024-2025/Summer 2024/fishics/data/data_for_henry_2024.csv\")\n",
    "data = data[data['date'] == '2/1/24']\n",
    "\n",
    "df = df.rename(columns={\"GPS.Latitude..\": \"lat\", \"GPS.Longitude..\": \"long\", \"Depth.m\":\"depth\",\"Sal.psu\":\"salt\",\"Temp..C\":\"temp\"})\n",
    "df = df[['lat','long','depth','salt','station','temp','pH']]\n",
    "\n",
    "# Group by station\n",
    "grouped = df.groupby('station')\n",
    "\n",
    "# Initialize lists to hold the aggregated data\n",
    "stations = []\n",
    "surface_temps = []\n",
    "surface_salts = []\n",
    "surface_pHs = []\n",
    "bottom_temps = []\n",
    "bottom_salts = []\n",
    "bottom_pHs = []\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "\n",
    "# Iterate over each group\n",
    "for station, group in grouped:\n",
    "    # Find the row with the minimum depth (surface)\n",
    "    min_depth_row = group.loc[group['depth'].idxmin()]\n",
    "    # Find the row with the maximum depth (bottom)\n",
    "    max_depth_row = group.loc[group['depth'].idxmax()]\n",
    "    \n",
    "    # Append the results to the lists\n",
    "    stations.append(station)\n",
    "    surface_temps.append(min_depth_row['temp'])\n",
    "    surface_salts.append(min_depth_row['salt'])\n",
    "    surface_pHs.append(min_depth_row['pH'])\n",
    "    bottom_temps.append(max_depth_row['temp'])\n",
    "    bottom_salts.append(max_depth_row['salt'])\n",
    "    bottom_pHs.append(max_depth_row['pH'])\n",
    "    latitudes.append(min_depth_row['lat'])  # assuming latitude is constant for each station\n",
    "    longitudes.append(min_depth_row['long'])  # assuming longitude is constant for each station\n",
    "\n",
    "aggregated_df = pd.DataFrame({\n",
    "    'station': stations,\n",
    "    'latitude': latitudes,\n",
    "    'longitude': longitudes,\n",
    "    'surface_temp': surface_temps,\n",
    "    'surface_salt': surface_salts,\n",
    "    'surface_pH': surface_pHs,\n",
    "    'bottom_temp': bottom_temps,\n",
    "    'bottom_salt': bottom_salts,\n",
    "    'bottom_pH': bottom_pHs\n",
    "})\n",
    "\n",
    "# Perform an inner join on the 'station' column\n",
    "merged_df = data.merge(aggregated_df, on='station', how='inner')\n",
    "fish_asvs = merged_df.iloc[:,11:86]\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=30, perplexity=20, learning_rate=10, n_iter=1000) #test with diff perplexity\n",
    "tsne_result = tsne.fit_transform(fish_asvs) # type: ignore\n",
    "merged_df['tSNE1'] = tsne_result[:, 0] # type: ignore\n",
    "merged_df['tSNE2'] = tsne_result[:, 1] # type: ignore\n",
    "# merged_df\n",
    "\n",
    "ml_df = merged_df.iloc[:,86:] #type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = ml_df # type: ignore\n",
    "df\n",
    "features = df.drop(columns=['tSNE1', 'tSNE2'])\n",
    "labels = df[['tSNE1', 'tSNE2']]\n",
    "# print(features.isnull().sum())\n",
    "\n",
    "labels = labels.loc[features.index]\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for tSNE1: 0.00012430342670020195\n",
      "R^2 Score for tSNE1: 0.26875407828753517\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on tSNE1\n",
    "rf_regressor.fit(X_train, y_train['tSNE1'])\n",
    "y_pred_tSNE1 = rf_regressor.predict(X_test)\n",
    "feature_importances_tSNE1 = rf_regressor.feature_importances_\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse_tSNE1 = mean_squared_error(y_test['tSNE1'], y_pred_tSNE1)\n",
    "r2_tSNE1 = r2_score(y_test['tSNE1'], y_pred_tSNE1)\n",
    "print(f\"Mean Squared Error for tSNE1: {mse_tSNE1}\")\n",
    "print(f\"R^2 Score for tSNE1: {r2_tSNE1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for tSNE2: 0.00035795611659953806\n",
      "R^2 Score for tSNE2: 0.09252491315272315\n"
     ]
    }
   ],
   "source": [
    "rf_regressor.fit(X_train, y_train['tSNE2'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_tSNE2 = rf_regressor.predict(X_test)\n",
    "feature_importances_tSNE2 = rf_regressor.feature_importances_\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse_tSNE2 = mean_squared_error(y_test['tSNE2'], y_pred_tSNE2)\n",
    "r2_tSNE2 = r2_score(y_test['tSNE2'], y_pred_tSNE2)\n",
    "\n",
    "print(f\"Mean Squared Error for tSNE2: {mse_tSNE2}\")\n",
    "print(f\"R^2 Score for tSNE2: {r2_tSNE2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importance_df_tSNE1 = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Importance': feature_importances_tSNE1\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# importance_df_tSNE2 = pd.DataFrame({\n",
    "#     'Feature': features.columns,\n",
    "#     'Importance': feature_importances_tSNE2\n",
    "# }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances for tSNE1\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df_tSNE1['Feature'], importance_df_tSNE1['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance for tSNE1')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# # Plot feature importances for tSNE2\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.barh(importance_df_tSNE2['Feature'], importance_df_tSNE2['Importance'])\n",
    "# plt.xlabel('Feature Importance')\n",
    "# plt.title('Feature Importance for tSNE2')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION MODEL\n",
    "\n",
    "Classification task\n",
    "Predict the presence/absence of a certain species - classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Atl_croaker_(nibea98)</th>\n",
       "      <th>Bay_anchovy</th>\n",
       "      <th>Black_drum_or_Spot</th>\n",
       "      <th>Black_sea_bass</th>\n",
       "      <th>Brd_striped_anchovy</th>\n",
       "      <th>Little_tunny_or_skipjack_tuna</th>\n",
       "      <th>Nor_sea_robin</th>\n",
       "      <th>Scup</th>\n",
       "      <th>Smallmouth_flounder</th>\n",
       "      <th>Southern_kingfish(nibea95)</th>\n",
       "      <th>...</th>\n",
       "      <th>White_catfish</th>\n",
       "      <th>Atl_salmon</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>surface_temp</th>\n",
       "      <th>surface_salt</th>\n",
       "      <th>surface_pH</th>\n",
       "      <th>bottom_temp</th>\n",
       "      <th>bottom_salt</th>\n",
       "      <th>bottom_pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240067</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.182897</td>\n",
       "      <td>-74.204214</td>\n",
       "      <td>7.207000</td>\n",
       "      <td>19.924286</td>\n",
       "      <td>7.894286</td>\n",
       "      <td>6.984000</td>\n",
       "      <td>22.357692</td>\n",
       "      <td>7.856154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010920</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.104352</td>\n",
       "      <td>-74.207144</td>\n",
       "      <td>7.572200</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>7.864000</td>\n",
       "      <td>7.327800</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>7.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.272691</td>\n",
       "      <td>-74.092105</td>\n",
       "      <td>7.175250</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>7.983750</td>\n",
       "      <td>7.034833</td>\n",
       "      <td>22.330000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>0.017985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.325567</td>\n",
       "      <td>-74.050293</td>\n",
       "      <td>7.141714</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>7.902857</td>\n",
       "      <td>6.951571</td>\n",
       "      <td>0.066429</td>\n",
       "      <td>7.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044081</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.215071</td>\n",
       "      <td>-74.129712</td>\n",
       "      <td>7.747545</td>\n",
       "      <td>21.470000</td>\n",
       "      <td>7.875455</td>\n",
       "      <td>7.162100</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013282</td>\n",
       "      <td>0.025096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015334</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.217271</td>\n",
       "      <td>-74.052457</td>\n",
       "      <td>7.412000</td>\n",
       "      <td>22.007000</td>\n",
       "      <td>7.865000</td>\n",
       "      <td>7.383020</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>7.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.216981</td>\n",
       "      <td>-74.440575</td>\n",
       "      <td>5.550700</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>7.836000</td>\n",
       "      <td>5.717857</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>7.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015507</td>\n",
       "      <td>0.038603</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036010</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.403917</td>\n",
       "      <td>-74.286663</td>\n",
       "      <td>4.890875</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.832500</td>\n",
       "      <td>5.516107</td>\n",
       "      <td>20.027500</td>\n",
       "      <td>7.783571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.051945</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.550527</td>\n",
       "      <td>-74.130042</td>\n",
       "      <td>6.065100</td>\n",
       "      <td>18.135000</td>\n",
       "      <td>7.849000</td>\n",
       "      <td>6.225100</td>\n",
       "      <td>21.723000</td>\n",
       "      <td>7.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008507</td>\n",
       "      <td>0.069150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.472763</td>\n",
       "      <td>-74.004055</td>\n",
       "      <td>6.678375</td>\n",
       "      <td>22.031250</td>\n",
       "      <td>7.835000</td>\n",
       "      <td>6.654333</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>7.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.068439</td>\n",
       "      <td>-74.127731</td>\n",
       "      <td>7.123857</td>\n",
       "      <td>22.711429</td>\n",
       "      <td>7.862857</td>\n",
       "      <td>7.379176</td>\n",
       "      <td>15.897059</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.018294</td>\n",
       "      <td>0.022694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.037855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.991007</td>\n",
       "      <td>-73.972367</td>\n",
       "      <td>7.699333</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>7.851667</td>\n",
       "      <td>7.938500</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.017572</td>\n",
       "      <td>0.024368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.103433</td>\n",
       "      <td>-73.964745</td>\n",
       "      <td>7.214500</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>7.888333</td>\n",
       "      <td>7.776800</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.364163</td>\n",
       "      <td>-73.896414</td>\n",
       "      <td>7.955429</td>\n",
       "      <td>20.768571</td>\n",
       "      <td>7.914286</td>\n",
       "      <td>7.200857</td>\n",
       "      <td>20.779286</td>\n",
       "      <td>7.849286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031501</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.288600</td>\n",
       "      <td>-73.739525</td>\n",
       "      <td>7.321667</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>7.858333</td>\n",
       "      <td>7.589667</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>7.824167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.476666</td>\n",
       "      <td>-73.891367</td>\n",
       "      <td>6.898286</td>\n",
       "      <td>21.882857</td>\n",
       "      <td>7.858571</td>\n",
       "      <td>7.142786</td>\n",
       "      <td>22.370000</td>\n",
       "      <td>7.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.012054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010711</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.472270</td>\n",
       "      <td>-73.740292</td>\n",
       "      <td>6.343000</td>\n",
       "      <td>11.487500</td>\n",
       "      <td>7.872500</td>\n",
       "      <td>7.461889</td>\n",
       "      <td>22.536667</td>\n",
       "      <td>7.853333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.013465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.734484</td>\n",
       "      <td>-73.972144</td>\n",
       "      <td>6.542500</td>\n",
       "      <td>19.558750</td>\n",
       "      <td>7.863750</td>\n",
       "      <td>6.364842</td>\n",
       "      <td>21.901579</td>\n",
       "      <td>7.845789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.660862</td>\n",
       "      <td>-73.896875</td>\n",
       "      <td>7.032000</td>\n",
       "      <td>21.691667</td>\n",
       "      <td>7.868333</td>\n",
       "      <td>6.526176</td>\n",
       "      <td>22.053529</td>\n",
       "      <td>7.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.663942</td>\n",
       "      <td>-73.740970</td>\n",
       "      <td>7.131667</td>\n",
       "      <td>17.976667</td>\n",
       "      <td>7.870000</td>\n",
       "      <td>6.400722</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.582736</td>\n",
       "      <td>-73.973833</td>\n",
       "      <td>6.693222</td>\n",
       "      <td>21.846667</td>\n",
       "      <td>7.864444</td>\n",
       "      <td>6.477400</td>\n",
       "      <td>22.030000</td>\n",
       "      <td>7.850000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Atl_croaker_(nibea98)  Bay_anchovy  Black_drum_or_Spot  Black_sea_bass  \\\n",
       "0                0.000000     0.240067            0.008431        0.000000   \n",
       "1                0.000000     0.010920            0.000726        0.000065   \n",
       "2                0.000000     0.000000            0.000000        0.000000   \n",
       "3                0.000000     0.075304            0.000000        0.000000   \n",
       "4                0.000000     0.158802            0.000000        0.000000   \n",
       "5                0.000000     0.013282            0.025096        0.000000   \n",
       "6                0.000000     0.008921            0.006158        0.000000   \n",
       "7                0.015507     0.038603            0.004211        0.000465   \n",
       "8                0.006269     0.051945            0.006176        0.000364   \n",
       "9                0.008507     0.069150            0.000000        0.000000   \n",
       "10               0.000000     0.000000            0.011916        0.000000   \n",
       "11               0.018294     0.022694            0.000000        0.010783   \n",
       "12               0.000000     0.021126            0.000000        0.000000   \n",
       "13               0.000000     0.000000            0.000000        0.000000   \n",
       "14               0.000000     0.031501            0.012988        0.003752   \n",
       "15               0.000000     0.009734            0.003974        0.000000   \n",
       "16               0.000000     0.024823            0.012054        0.000000   \n",
       "17               0.000000     0.013080            0.013465        0.000000   \n",
       "18               0.000000     0.189419            0.000000        0.000052   \n",
       "19               0.000000     0.000000            0.009373        0.000000   \n",
       "20               0.000000     0.011171            0.000000        0.001862   \n",
       "\n",
       "    Brd_striped_anchovy  Little_tunny_or_skipjack_tuna  Nor_sea_robin  \\\n",
       "0              0.000000                            0.0       0.005233   \n",
       "1              0.000000                            0.0       0.000610   \n",
       "2              0.000000                            0.0       0.028369   \n",
       "3              0.000000                            0.0       0.000000   \n",
       "4              0.000000                            0.0       0.044081   \n",
       "5              0.000000                            0.0       0.008265   \n",
       "6              0.000000                            0.0       0.018742   \n",
       "7              0.000000                            0.0       0.001601   \n",
       "8              0.000000                            0.0       0.000168   \n",
       "9              0.000000                            0.0       0.000000   \n",
       "10             0.000000                            0.0       0.000000   \n",
       "11             0.000000                            0.0       0.000000   \n",
       "12             0.000000                            0.0       0.008899   \n",
       "13             0.000000                            0.0       0.000000   \n",
       "14             0.000000                            0.0       0.000783   \n",
       "15             0.006435                            0.0       0.003774   \n",
       "16             0.000000                            0.0       0.006187   \n",
       "17             0.000000                            0.0       0.003142   \n",
       "18             0.000000                            0.0       0.000083   \n",
       "19             0.000000                            0.0       0.000000   \n",
       "20             0.000000                            0.0       0.000416   \n",
       "\n",
       "        Scup  Smallmouth_flounder  Southern_kingfish(nibea95)  ...  \\\n",
       "0   0.002279             0.006085                    0.003435  ...   \n",
       "1   0.000182             0.031328                    0.000000  ...   \n",
       "2   0.000000             0.000000                    0.000000  ...   \n",
       "3   0.016420             0.017985                    0.000000  ...   \n",
       "4   0.014342             0.000000                    0.000000  ...   \n",
       "5   0.000000             0.015334                    0.004269  ...   \n",
       "6   0.000000             0.036051                    0.000000  ...   \n",
       "7   0.000000             0.036010                    0.000402  ...   \n",
       "8   0.000000             0.033119                    0.000000  ...   \n",
       "9   0.000000             0.047961                    0.000000  ...   \n",
       "10  0.000000             0.141404                    0.000000  ...   \n",
       "11  0.016612             0.037855                    0.000000  ...   \n",
       "12  0.017572             0.024368                    0.000000  ...   \n",
       "13  0.000000             0.016927                    0.007816  ...   \n",
       "14  0.000000             0.008803                    0.008411  ...   \n",
       "15  0.000000             0.000000                    0.000000  ...   \n",
       "16  0.000000             0.010711                    0.009848  ...   \n",
       "17  0.001346             0.005450                    0.000000  ...   \n",
       "18  0.000000             0.000000                    0.000000  ...   \n",
       "19  0.007629             0.000000                    0.000000  ...   \n",
       "20  0.000000             0.000000                    0.000000  ...   \n",
       "\n",
       "    White_catfish  Atl_salmon   latitude  longitude  surface_temp  \\\n",
       "0             0.0         0.0  39.182897 -74.204214      7.207000   \n",
       "1             0.0         0.0  39.104352 -74.207144      7.572200   \n",
       "2             0.0         0.0  39.272691 -74.092105      7.175250   \n",
       "3             0.0         0.0  39.325567 -74.050293      7.141714   \n",
       "4             0.0         0.0  39.215071 -74.129712      7.747545   \n",
       "5             0.0         0.0  39.217271 -74.052457      7.412000   \n",
       "6             0.0         0.0  39.216981 -74.440575      5.550700   \n",
       "7             0.0         0.0  39.403917 -74.286663      4.890875   \n",
       "8             0.0         0.0  39.550527 -74.130042      6.065100   \n",
       "9             0.0         0.0  39.472763 -74.004055      6.678375   \n",
       "10            0.0         0.0  39.068439 -74.127731      7.123857   \n",
       "11            0.0         0.0  38.991007 -73.972367      7.699333   \n",
       "12            0.0         0.0  39.103433 -73.964745      7.214500   \n",
       "13            0.0         0.0  39.364163 -73.896414      7.955429   \n",
       "14            0.0         0.0  39.288600 -73.739525      7.321667   \n",
       "15            0.0         0.0  39.476666 -73.891367      6.898286   \n",
       "16            0.0         0.0  39.472270 -73.740292      6.343000   \n",
       "17            0.0         0.0  39.734484 -73.972144      6.542500   \n",
       "18            0.0         0.0  39.660862 -73.896875      7.032000   \n",
       "19            0.0         0.0  39.663942 -73.740970      7.131667   \n",
       "20            0.0         0.0  39.582736 -73.973833      6.693222   \n",
       "\n",
       "    surface_salt  surface_pH  bottom_temp  bottom_salt  bottom_pH  \n",
       "0      19.924286    7.894286     6.984000    22.357692   7.856154  \n",
       "1       0.060000    7.864000     7.327800     0.037500   7.852500  \n",
       "2      18.300000    7.983750     7.034833    22.330000   7.850000  \n",
       "3       0.008571    7.902857     6.951571     0.066429   7.870000  \n",
       "4      21.470000    7.875455     7.162100    22.380000   7.850000  \n",
       "5      22.007000    7.865000     7.383020    22.400000   7.860000  \n",
       "6       0.083000    7.836000     5.717857     0.105000   7.830000  \n",
       "7      19.000000    7.832500     5.516107    20.027500   7.783571  \n",
       "8      18.135000    7.849000     6.225100    21.723000   7.825000  \n",
       "9      22.031250    7.835000     6.654333    22.150000   7.840000  \n",
       "10     22.711429    7.862857     7.379176    15.897059   7.850000  \n",
       "11      0.051667    7.851667     7.938500     0.028000   7.850000  \n",
       "12      0.028333    7.888333     7.776800     0.041000   7.850000  \n",
       "13     20.768571    7.914286     7.200857    20.779286   7.849286  \n",
       "14      0.038333    7.858333     7.589667     0.087500   7.824167  \n",
       "15     21.882857    7.858571     7.142786    22.370000   7.830000  \n",
       "16     11.487500    7.872500     7.461889    22.536667   7.853333  \n",
       "17     19.558750    7.863750     6.364842    21.901579   7.845789  \n",
       "18     21.691667    7.868333     6.526176    22.053529   7.852941  \n",
       "19     17.976667    7.870000     6.400722    22.000000   7.860000  \n",
       "20     21.846667    7.864444     6.477400    22.030000   7.850000  \n",
       "\n",
       "[21 rows x 83 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df2 = merged_df.iloc[:,11:] # type: ignore\n",
    "ml_df2 = ml_df2.drop(columns = ['tSNE1', 'tSNE2'])\n",
    "ml_df2\n",
    "\n",
    "## FILTER FOR eDNA values + OCEANOGRAPHIC DATA. REMOVE tSNE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Confusion Matrix:\n",
      " [[4 0]\n",
      " [1 0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrysun_1/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/henrysun_1/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/henrysun_1/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "features = ml_df2[['latitude', 'longitude', 'surface_temp', 'surface_salt', 'bottom_temp', 'bottom_salt', 'surface_pH', 'bottom_pH']]\n",
    "labels = ml_df2['Atl_croaker_(nibea98)'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the multi-output Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# # Evaluate the model\n",
    "# for i, species in enumerate(labels.columns):\n",
    "#     print(f\"Classification report for {species}:\\n\", classification_report(y_test.iloc[:, i], y_pred[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAGDCAYAAABZbbppAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr9UlEQVR4nO3de5glVX3v//eHGRzugzpoBoy0AlERZJABb8BBMYk/SRCPJKiIYIgkqInIYxK85RA1BkMSieaXYzhGUCBoREiI/EQJApIJtxmYC4igwngUiHeHqwSY7++PvbrYttMz3dPds7t73q/n2U/XXlVr1apFdc+HVVV7p6qQJEmSALYYdAckSZI0fRgOJUmS1DEcSpIkqWM4lCRJUsdwKEmSpI7hUJIkSR3DoSRp4JKcneSDg+6HJMOhpGkgyeokDyW5v++18yS0+fLJ6uMY9ndqknM31f7WJ8lxSf5j0P0YKcmVSX7W/vv+MMmFSRZuRDuVZPep6KMkw6Gk6eM3q2q7vtfdg+xMkrmD3P/GmgH9fltVbQf8CrAj8JHBdkfSSIZDSdNWkvlJ/jHJPUnuSvLBJHPaut2SfCXJj9os1HlJdmzrzgGeDvxbm6X64ySHJPnuiPa72cU283dBknOT3Asct779j6HvleQtSb6R5L4kH2h9/s8k9yb55yRPaNsekuS7Sd7djmV1kqNHjMOnk/wgybeTvDfJFm3dcUmWJPlIkh8BnwU+DryoHftP23aHJbmp7fs7SU7ta3+o9ffYJP+39eE9fevntL59qx3LsiS/3NY9O8llSX6c5LYkvz2W8amqHwOfB/YaZfzenOSbrd2Lh2eSk3y1bbKiHd9RY9mfpLEzHEqazs4GHgV2B/YFfg343bYuwF8AOwPPAX4ZOBWgqo4B/i+Pz0b+5Rj39yrgAnozWudtYP9j8evAfsALgT8GzgTe0Pq6F/C6vm1/CVgA7AIcC5yZ5Flt3ceA+cAzgf8BvBF4U1/dFwB3AE9t7f8+cE079h3bNg+0ejsChwEnJjliRH8PBJ4FHAr8aZLntPKTW19fCewA/A7wYJJtgcuAfwKeArwW+Pske25oYJIsAF4D3LSOdS+j99/2t4GFwLeBzwBU1cFts33a8X12Q/uSND6GQ0nTxb8k+Wl7/UuSp9ILIydV1QNV9X16lyBfC1BV36yqy6rq4ar6AfA39ILTRFxTVf9SVWvphaBR9z9Gf1lV91bVLcDNwJer6o6qWgN8kV7g7Pe+djxXAZcAv91mKl8LvKuq7quq1cBfA8f01bu7qj5WVY9W1UPr6khVXVlVq6pqbVWtBM7nF8frz6rqoapaAawA9mnlvwu8t6puq54VVfUj4DeA1VV1Vtv3TfRmA39rPWPy0TabuQK4h17wHOlo4JNVdWNVPQy8i95M6NB62pU0Sab7vSmSNh9HVNW/D79JcgCwJXBPkuHiLYDvtPVPBf4WOAjYvq37yQT78J2+5V3Xt/8x+l7f8kPreP9Lfe9/UlUP9L3/Nr1Z0QWtH98esW6XUfq9TkleAJxGb8byCcA84HMjNvuvvuUHge3a8i8D31pHs7sCLxi+dN3MBc5ZT1f+sKo+sYHu7gzcOPymqu5vl8x3AVZvoK6kCXLmUNJ09R3gYWBBVe3YXjtU1XPb+g8BBexdVTvQu5yavvo1or0HgG2G37QZuZ1GbNNfZ0P7n2xPbJdphz0duBv4IfAIvSDWv+6uUfq9rvfQu/R7MfDLVTWf3n2JWcd26/IdYLdRyq/qG58d26XeE8fY7mjupu9427g8mZ8/ZklTxHAoaVqqqnuALwN/nWSHJFu0BzqGL4VuD9wPrEmyC/BHI5r4Hr179IbdDmzVHszYEngvvdmzjd3/VPizJE9IchC9S7afq6rHgH8G/jzJ9kl2pXcpdn0fm/M94GnDD7w02wM/rqqftVnZ14+jX58APpBkj/Q8L8mTgS8Av5LkmCRbttf+ffcqbqzzgTclWZRkHr3/EbiuXVIfPr5njlZZ0sQYDiVNZ2+kdwn0a/QuGV9A7wEFgD8Dng+soXd/3oUj6v4F8N52D+M7231+b6EXdO6iN5P4XdZvffufbP/V9nE3vYdhfr+qvt7W/QG9/t4B/Ae9WcBPrqetrwC3AP+V5Iet7C3A+5PcB/wpvcA5Vn/Ttv8ycC/wj8DWVXUfvYd0Xtv6/V/Ah1lP6B6LdnvB++jdv3gPvVnL/ns9TwU+1f7bjunpaEljl6p1XX2QJG0qSQ4Bzq2qpw24K5LkzKEkSZIeZziUJElSx8vKkiRJ6jhzKEmSpI7hUJIkSR2/IWWSLFiwoIaGhgbdDUmSpA1atmzZD6tq5BcBAIbDSTM0NMTSpUsH3Q1JkqQNSvLt0dZ5WVmSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSx3AoSZKkjuFQkiRJHcOhJEmSOoZDSZIkdQyHkiRJ6swddAdmi1V3rWHolEsG3Q1pIFafdtiguyBJmiTOHEqSJKljOJQkSVLHcChJkqSO4VCSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSx3AoSZKkjuFQkiRJnYGHwyTnJ1mZ5B2D7suwJKcmeWdbPi7JzoPukyRJ0qYwd1A7TjIXWADsX1W7D6ofY3AccDNw94D7IUmSNOUmPHOYZNsklyRZkeTmJEclWZ1kQVu/OMmVbfnUJOckWQKcA3wZ2CXJ8iQHJXlzkhtaW59Psk2r99QkF7XyFUle3MrfkOT6Vv8fkswZpY9zkpzd+rdqeJZytP311TsSWAyc1/ax9UTHS5IkaTqbjMvKrwDurqp9qmov4NINbL8n8PKqeh1wOPCtqlpUVVcDF1bV/lW1D3ArcHyr81Hgqlb+fOCWJM8BjgJeUlWLgMeAo0fZ5yJgl6raq6r2Bs5q5aPtD4CqugBYChzd+vhQ//okJyRZmmTpYw+u2cBhS5IkTX+TEQ5XAb+a5MNJDqqqDaWki0eGrD57Jbk6ySp6Qe+5rfxlwP8GqKrH2j4OBfYDbkiyvL1/5ijt3gE8M8nHkrwCuHcD+xuTqjqzqhZX1eI528wfT1VJkqRpacL3HFbV7UmeD7wS+GCSy4FHeTx4bjWiygPrae5s4IiqWpHkOOCQ9Wwb4FNV9a4x9PEnSfYBfh34feC3gd8Z5/4kSZJmvcm453Bn4MGqOhc4nd5l39X0ZvUAXjOO5rYH7kmyJT9/ifhy4MS2vzlJ5reyI5M8pZU/Kcmuo/RxAbBFVX0eeG/r4/r21+++tp0kSdKsNxlPK+8NnJ5kLfAIvRC3NfCPST4AXDmOtt4HXAf8oP0cDmVvB85Mcjy9ewtPrKprkrwX+HKSLdq+3wp8ex3t7gKc1bYDGJ5tHG1//c4GPp7kIeBF67kkLkmSNOOlqgbdh1lh3sI9auGxZwy6G9JArD7tsEF3QZI0DkmWVdXida0b+IdgS5IkafoY2IdgT5Uk1wHzRhQfU1WrBtEfSZKkmWTWhcOqesGg+yBJkjRTeVlZkiRJHcOhJEmSOoZDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqTPrPspmUPbeZT5L/ZYISZI0wzlzKEmSpI7hUJIkSR3DoSRJkjqGQ0mSJHUMh5IkSeoYDiVJktTxo2wmyaq71jB0yiWD7oakaWi1H3MlaQZx5lCSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSx3AoSZKkjuFQkiRJHcOhJEmSOoZDSZIkdQyHkiRJ6ow5HCYZSnLzOLY/Kck2fe/fPd7OjUeSQ5K8eCr3IUmSNNtN5czhScA2fe+nNBwChwCGQ0mSpAkYbzicm+S8JLcmuSDJNkkOTXJTklVJPplkXpI/BHYGrkhyRZLTgK2TLE9yHkCSk5Pc3F4ntbKhJF9PcnaS29u+Xp5kSZJvJDlgXZ1KMgT8PvCOto+DkuyU5PNJbmivl7RtT03yqSRXJ/l2kv+Z5C9b/y9NsmXbbnVf+fVJdt+YAZYkSZpJxhsOnwX8fVU9B7gXOBk4GziqqvYG5gInVtVHgbuBl1bVS6vqFOChqlpUVUcn2Q94E/AC4IXAm5Ps2/axO/DXwLPb6/XAgcA7GWX2sapWAx8HPtL2cTXwt+39/sBrgE/0VdkNeBlwOHAucEXr/0PAYX3brWnlfwecMXK/SU5IsjTJ0sceXDOmAZQkSZrOxhsOv1NVS9ryucChwJ1VdXsr+xRw8BjaORC4qKoeqKr7gQuBg9q6O6tqVVWtBW4BLq+qAlYBQ+Po68uBv0uyHLgY2CHJdm3dF6vqkdbmHODSVj5yH+f3/XzRyB1U1ZlVtbiqFs/ZZv44uiZJkjQ9zR3n9jXi/U+BJ09OVzoP9y2v7Xu/lvH1dwvghVX1s/7CJN0+qmptkkda+FzXPmqUZUmSpFlpvDOHT08yPIP2emApMNR3P94xwFVt+T5g+766jwzfzwdcDRzR7lncFnh1K5uIkfv7MvAHw2+SLNqINo/q+3nNRvdMkiRphhhvOLwNeGuSW4EnAh+hd+/g55Ksojfz9vG27ZnApUmu6Hu/Msl5VXUjvXsVrweuAz5RVTdN6Ejg34BXDz+QAvwhsDjJyiRfo/fAyng9MclK4O3AOybYP0mSpGkvj19RVb8kq4HFVfXDsWw/b+EetfDYM6a0T5JmptWnHbbhjSRpE0qyrKoWr2ud35AiSZKkzngfSBm4JG+id5m335Kqeutk7qeqhiazPUmSpJlgxoXDqjoLOGvQ/ZAkSZqNvKwsSZKkjuFQkiRJHcOhJEmSOoZDSZIkdQyHkiRJ6hgOJUmS1JlxH2UzXe29y3yW+i0IkiRphnPmUJIkSR3DoSRJkjqGQ0mSJHUMh5IkSeoYDiVJktQxHEqSJKnjR9lMklV3rWHolEsG3Q1JM9BqPwZL0jTizKEkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSx3AoSZKkjuFQkiRJHcOhJEmSOoZDSZIkdQyHkiRJ6hgOJUmS1JmUcJhkKMnN49j+pCTb9L1/92T0Y7Ikub/9HEry+kH3R5IkaVMZ1MzhScA2fe+nVTjsMwQYDiVJ0mZjMsPh3CTnJbk1yQVJtklyaJKbkqxK8skk85L8IbAzcEWSK5KcBmydZHmS8wCSnJzk5vY6qZUNJfl6krOT3N729fIkS5J8I8kBo3Usyf9o7S9v/dk+yXZJLk9yY+vfq9ZR9TTgoFbvHZM4VpIkSdNSqmrijSRDwJ3AgVW1JMkngTuA3wMOrarbk3wauLGqzkiyGlhcVT9s9e+vqu3a8n7A2cALgQDXAW8AfgJ8E9gXuAW4AVgBHA8cDrypqo4YpX//BpzW+rYd8LO2apuqujfJAuBaYI+qquH+JDkEeGdV/cYo7Z4AnAAwZ4ed9nvaiWeNf/AkbfZWn3bYoLsgaTOTZFlVLV7XusmcOfxOVS1py+cChwJ3VtXtrexTwMFjaOdA4KKqeqCq7gcuBA5q6+6sqlVVtZZeQLy8eul2Fb1LwKNZAvxNm7XcsaoepRc8P5RkJfDvwC7AU8d4rABU1ZlVtbiqFs/ZZv54qkqSJE1LkxkOR05B/nQS2x72cN/y2r73a4G5o1WqqtOA3wW2BpYkeTZwNLATsF9VLQK+B2w1BX2WJEmaMSYzHD49yYva8uuBpcBQkt1b2THAVW35PmD7vrqPJNmyLV8NHNHuWdwWeHUr22hJdmszjh+mdzn62cB84PtV9UiSlwK7rqPqyH5KkiTNapMZDm8D3prkVuCJwEeANwGfS7KK3uzex9u2ZwKXJrmi7/3KJOdV1Y307jm8nt79hp+oqpsm2LeT2sMtK4FHgC8C5wGLW9/eCHx9HfVWAo8lWeEDKZIkaXMwKQ+kCOYt3KMWHnvGoLshaQbygRRJm9qmeiBFkiRJM9yoD3HMREneBLx9RPGSqnrrIPojSZI008yqcFhVZwF+2KAkSdJG8rKyJEmSOoZDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqWM4lCRJUmdWfZTNIO29y3yW+i0HkiRphnPmUJIkSR3DoSRJkjqGQ0mSJHUMh5IkSeoYDiVJktQxHEqSJKljOJQkSVLHzzmcJKvuWsPQKZcMuhuSBMBqP3dV0kZy5lCSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSx3AoSZKkjuFQkiRJHcOhJEmSOoZDSZIkdaYsHCYZSnLzOLY/Kck2fe/fPTU922A/fqHfSU5N8s5B9EeSJGlTmk4zhycB2/S9H0g4lCRJ2pxNdTicm+S8JLcmuSDJNkkOTXJTklVJPplkXpI/BHYGrkhyRZLTgK2TLE9yHkCSk5Pc3F4ntbKhJF9PcnaS29u+Xp5kSZJvJDlgtI612cBzklzTtn3zFI+FJEnStDd3itt/FnB8VS1J8kngZOD3gEOr6vYknwZOrKozkpwMvLSqfgiQ5G1Vtagt7we8CXgBEOC6JFcBPwF2B34L+B3gBuD1wIHA4fRmH49YT/+eB7wQ2Ba4KcklrXy3JMv7tvsl4K9GVk5yAnACwJwddhrHsEiSJE1PUz1z+J2qWtKWzwUOBe6sqttb2aeAg8fQzoHARVX1QFXdD1wIHNTW3VlVq6pqLXALcHlVFbAKGNpAu/9aVQ+1QHoFMDzT+K2qWjT8Aj6+rspVdWZVLa6qxXO2mT+Gw5AkSZrepjoc1oj3P52CfTzct7y27/1aNjwzOrJ/I99LkiRtVqY6HD49yYva8uuBpcBQkt1b2THAVW35PmD7vrqPJNmyLV8NHNHuWdwWeHUrm6hXJdkqyZOBQ+hdlpYkSdpsTXU4vA14a5JbgScCH6F37+DnkqyiN7s3fMn2TODSJFf0vV+Z5LyquhE4G7geuA74RFXdNAn9W0nvcvK1wAeq6u5JaFOSJGnGSu/2vM1PklOB+6vqFx402RjzFu5RC489YzKakqQJW33aYYPugqRpLMmyqlq8rnXT6XMOJUmSNGBT/VE2A5fkTcDbRxQvqaq3DqI/kiRJ09msD4dVdRZw1qD7IUmSNBN4WVmSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpM+s/ymZT2XuX+Sz1GwkkSdIM58yhJEmSOoZDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqWM4lCRJUsdwKEmSpI6fczhJVt21hqFTLhl0NyRpQlb7ea3SZs+ZQ0mSJHUMh5IkSeoYDiVJktQxHEqSJKljOJQkSVLHcChJkqSO4VCSJEkdw6EkSZI6hkNJkiR1DIeSJEnqDCQcJrl/Cto8PMkpbfmIJHtuRBtXJlk82X2TJEmaKWbNzGFVXVxVp7W3RwDjDoeSJEmbu4GGw/ScnuTmJKuSHNXKD2mzeBck+XqS85KkrXtlK1uW5KNJvtDKj0vyd0leDBwOnJ5keZLd+mcEkyxIsrotb53kM0luTXIRsHVf334tyTVJbkzyuSTbbdrRkSRJ2vTmDnj//xNYBOwDLABuSPLVtm5f4LnA3cAS4CVJlgL/ABxcVXcmOX9kg1X1n0kuBr5QVRcAtFy5LicCD1bVc5I8D7ixbb8AeC/w8qp6IMmfACcD75+EY5YkSZq2Bh0ODwTOr6rHgO8luQrYH7gXuL6qvguQZDkwBNwP3FFVd7b65wMnTGD/BwMfBaiqlUlWtvIX0rssvaQFyycA14ysnOSE4f3P2WGnCXRDkiRpehh0OFyfh/uWH2NifX2Uxy+hbzWG7QNcVlWvW99GVXUmcCbAvIV71AT6J0mSNC0M+oGUq4GjksxJshO9mbzr17P9bcAzkwy190eNst19wPZ971cD+7XlI/vKvwq8HiDJXsDzWvm19C5j797WbZvkV8ZyQJIkSTPZoMPhRcBKYAXwFeCPq+q/Rtu4qh4C3gJcmmQZvRC4Zh2bfgb4oyQ3JdkN+CvgxCQ30bu3cdj/BrZLciu9+wmXtf38ADgOOL9dar4GePZEDlSSJGkmSNXMuhqaZLuqur89vfz/At+oqo8Mul/zFu5RC489Y9DdkKQJWX3aYYPugqRNIMmyqlrnZzsPeuZwY7y5PaByCzCf3tPLkiRJmgTT+YGUdWqzhAOfKZQkSZqNZuLMoSRJkqaI4VCSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSZ8Z9CPZ0tfcu81nq105JkqQZzplDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqWM4lCRJUsdwKEmSpI7hUJIkSR0/53CSrLprDUOnXDLobkjSZm+1nzkrTYgzh5IkSeoYDiVJktQxHEqSJKljOJQkSVLHcChJkqSO4VCSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUmbbhMMn9G1i/Y5K39L3fOckFbXlRklduxD5PTfLO8fdWkiRpdpi24XAMdgS6cFhVd1fVke3tImDc4VCSJGlzN+3DYZLtklye5MYkq5K8qq06DdgtyfIkpycZSnJzkicA7weOauuOGjkj2LYbasvvSXJ7kv8AntW3zW5JLk2yLMnVSZ696Y5akiRpMOYOugNj8DPg1VV1b5IFwLVJLgZOAfaqqkUAw2Gvqv47yZ8Ci6vqbW3dqetqOMl+wGvpzTTOBW4ElrXVZwK/X1XfSPIC4O+Bl03FAUqSJE0XMyEcBvhQkoOBtcAuwFMnqe2DgIuq6kGAFjpJsh3wYuBzSYa3nfcLHUtOAE4AmLPDTpPUJUmSpMGZCeHwaGAnYL+qeiTJamCrcbbxKD9/CX1D9bcAfjo8KzmaqjqT3gwj8xbuUePskyRJ0rQz7e85BOYD32/B8KXArq38PmD7UeqMXLcaeD5AkucDz2jlXwWOSLJ1ku2B3wSoqnuBO5P8VquTJPtM3iFJkiRNTzMhHJ4HLE6yCngj8HWAqvoRsKQ9XHL6iDpXAHsOP5ACfB54UpJbgLcBt7c2bgQ+C6wAvgjc0NfG0cDxSVYAtwCvQpIkaZZLlVdDJ8O8hXvUwmPPGHQ3JGmzt/q0wwbdBWnaS7Ksqhava91MmDmUJEnSJmI4lCRJUsdwKEmSpI7hUJIkSR3DoSRJkjqGQ0mSJHUMh5IkSeoYDiVJktQxHEqSJKljOJQkSVJn7qA7MFvsvct8lvqVTZIkaYZz5lCSJEkdw6EkSZI6hkNJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSx885nCSr7lrD0CmXDLobkqTN0Go/Z1eTyJlDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqWM4lCRJUsdwKEmSpI7hUJIkSR3DoSRJkjqGQ0mSJHUMh5IkSeoYDiVJktSZ8nCY5PwkK5O8Y4r38+6pbF+SJGlzMHeqGk4yF1gA7F9Vu0/Vfvq8G/jQJtiPJEnSrLXBmcMk2ya5JMmKJDcnOSrJ6iQL2vrFSa5sy6cmOSfJEuAc4MvALkmWJzkoyZuT3NDa+nySbVq9pya5qJWvSPLiVv6GJNe3+v+QZM4ofTwN2Lptd9766ia5P8npSW5J8u9JDkhyZZI7khzetjkuyb+28m8k+V8THGdJkqQZYSyXlV8B3F1V+1TVXsClG9h+T+DlVfU64HDgW1W1qKquBi6sqv2rah/gVuD4VuejwFWt/PnALUmeAxwFvKSqFgGPAUeva4dVdQrwUNvP0Ruouy3wlap6LnAf8EHgV4FXA+/va/YA4DXA84DfSrJ45H6TnJBkaZKljz24ZgPDIkmSNP2N5bLyKuCvk3wY+EJVXZ1kfdtfXFUPjbJuryQfBHYEtgO+1MpfBrwRoKoeA9YkOQbYD7ih7W9r4Ptj6C/Aoeup+988HnBXAQ9X1SNJVgFDfW1cVlU/AkhyIXAgsLR/J1V1JnAmwLyFe9QY+yZJkjRtbTAcVtXtSZ4PvBL4YJLLgUd5fNZxqxFVHlhPc2cDR1TViiTHAYesZ9sAn6qqd22oj+Os+0hVDQe5tcDDAFW1tt0nOWxk2DP8SZKkWW8s9xzuDDxYVecCp9O77Lua3swc9C69jtX2wD1JtuTnLxFfDpzY9jcnyfxWdmSSp7TyJyXZdT1tP9LaHW5vPHXX5Vdbva2BI4Al46wvSZI044zlnsO9geuTLAf+F7179P4M+NskS+ndzzdW7wOuoxe0vt5X/nbgpe3S7jJgz6r6GvBe4MtJVgKXAQvX0/aZwMok521E3XW5Hvg8sBL4fFUt3cD2kiRJM14ev8KqYe2S9+KqettY68xbuEctPPaMKeuTJEmjWX3aYYPugmaYJMuq6hcetgW/IUWSJEl9puxDsKdKkuuAeSOKj6mqVZO1j6o6m97DM5IkSZuVGRcOq+oFg+6DJEnSbOVlZUmSJHUMh5IkSeoYDiVJktQxHEqSJKljOJQkSVLHcChJkqTOjPsom+lq713ms9RPqJckSTOcM4eSJEnqGA4lSZLUMRxKkiSpYziUJElSx3AoSZKkjuFQkiRJHcOhJEmSOn7O4SRZddcahk65ZNDdkCRJM9jqafCZyc4cSpIkqWM4lCRJUsdwKEmSpI7hUJIkSR3DoSRJkjqGQ0mSJHUMh5IkSeoYDiVJktQxHEqSJKljOJQkSVLHcChJkqTOtAuHSc5PsjLJOwa0/7OTHDmi7P5B9EWSJGlTmzvoDgxLMhdYAOxfVbsPuj+SJEmbo0mfOUyybZJLkqxIcnOSo5KsTrKgrV+c5Mq2fGqSc5IsAc4BvgzskmR5koOSvDnJDa2tzyfZptV7apKLWvmKJC9u5W9Icn2r/w9J5qynn/cn+UiSW5JcnmSnyR4LSZKkmWYqLiu/Ari7qvapqr2ASzew/Z7Ay6vqdcDhwLeqalFVXQ1cWFX7V9U+wK3A8a3OR4GrWvnzgVuSPAc4CnhJVS0CHgOOXs9+twWWVtVzgauA/9W37vQWMJcnWT5aA0lOSLI0ydLHHlyzgcOUJEma/qbisvIq4K+TfBj4QlVdnWR9219cVQ+Nsm6vJB8EdgS2A77Uyl8GvBGgqh4D1iQ5BtgPuKHtb2vg++vZ71rgs235XODCvnV/VFUXDL8Z7Z7DqjoTOBNg3sI9aj37kiRJmhEmPRxW1e1Jng+8EvhgksuBR3l8lnKrEVUeWE9zZwNHVNWKJMcBh6xn2wCfqqp3bUy/AcOdJEna7E3FPYc7Aw9W1bnA6fQu+66mN6sH8JpxNLc9cE+SLfn5S8SXAye2/c1JMr+VHZnkKa38SUl2XU/bWwDDTyW/HviPcfRLkiRpVpqKy8p707tnby3wCL0QtzXwj0k+AFw5jrbeB1wH/KD93L6Vvx04M8nx9O4tPLGqrknyXuDLSbZo+34r8O1R2n4AOKDV+T69+xUlSZI2a6naPK+mJrm/qrabrPbmLdyjFh57xmQ1J0mSNkOrTztsk+wnybKqWryuddPuQ7AlSZI0ONPmQ7CnSpLrgHkjio+ZzFlDSZKk2WLWh8OqesGg+yBJkjRTeFlZkiRJHcOhJEmSOoZDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqTPrP8pmU9l7l/ks3USfai5JkjRVnDmUJElSx3AoSZKkjuFQkiRJHcOhJEmSOoZDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqWM4lCRJUsdwKEmSpI7hUJIkSR3DoSRJkjqGQ0mSJHUMh5IkSeqkqgbdh1khyX3AbYPuxzSwAPjhoDsxDTgOPY6DYzDMcehxHHoch55BjsOuVbXTulbM3dQ9mcVuq6rFg+7EoCVZ6jg4DsMcB8dgmOPQ4zj0OA4903UcvKwsSZKkjuFQkiRJHcPh5Dlz0B2YJhyHHsehx3FwDIY5Dj2OQ4/j0DMtx8EHUiRJktRx5lCSJEkdw+EokrwiyW1JvpnklHWsn5fks239dUmG+ta9q5XfluTXx9rmdLOxY5DkV5MsS7Kq/XxZX50rW5vL2+spm/CQNsoExmEoyUN9x/rxvjr7tfH5ZpKPJskmPKSNMoFxOLpvDJYnWZtkUVs3G8+Hg5PcmOTRJEeOWHdskm+017F95TPqfNjYMUiyKMk1SW5JsjLJUX3rzk5yZ9+5sGgTHc5Gm+C58FjfsV7cV/6M9vvzzfb79IRNcSwTMYHz4aUj/jb8LMkRbd1sPB9OTvK1du5fnmTXvnXT629DVfka8QLmAN8Cngk8AVgB7Dlim7cAH2/LrwU+25b3bNvPA57R2pkzljan02uCY7AvsHNb3gu4q6/OlcDiQR/fJhqHIeDmUdq9HnghEOCLwP8z6GOdqnEYsc3ewLdm+fkwBDwP+DRwZF/5k4A72s8ntuUnzrTzYYJj8CvAHm15Z+AeYMf2/uz+baf7ayLj0NbdP0q7/wy8ti1/HDhx0Mc6lePQt82TgB8D28zi8+Glfcd3Io//WzHt/jY4c7huBwDfrKo7quq/gc8ArxqxzauAT7XlC4BDW6J/FfCZqnq4qu4EvtnaG0ub08lGj0FV3VRVd7fyW4Ctk8zbJL2efBM5F9YpyUJgh6q6tnq//Z8Gjpj0nk+uyRqH17W6M9UGx6GqVlfVSmDtiLq/DlxWVT+uqp8AlwGvmIHnw0aPQVXdXlXfaMt3A98H1vkhvDPARM6FdWq/Ly+j9/sDvd+nIyatx1NjssbhSOCLVfXg1HV1So1lHK7oO75rgae15Wn3t8FwuG67AN/pe//dVrbObarqUWAN8OT11B1Lm9PJRMag32uAG6vq4b6ys9plgvdN98tnTHwcnpHkpiRXJTmob/vvbqDN6WayzoejgPNHlM2282G8dWfa+TApf8uSHEBvhuVbfcV/3i65fWQG/A/lRMdhqyRLk1w7fCmV3u/LT9vvz8a0OQiT9W/ba/nFvw2z+Xw4nt5M4PrqDuxvg+FQUybJc4EPA7/XV3x0Ve0NHNRexwyib5vIPcDTq2pf4GTgn5LsMOA+DUySFwAPVtXNfcWb0/mgps2InAO8qaqGZ5PeBTwb2J/e5bU/GVD3NpVdq/fNGK8Hzkiy26A7NCjtfNgb+FJf8aw9H5K8AVgMnD7ovozGcLhudwG/3Pf+aa1sndskmQvMB360nrpjaXM6mcgYkORpwEXAG6uqmxmoqrvaz/uAf6I3FT+dbfQ4tFsLfgRQVcvozZD8Stv+aX31p/u5ABM8H5pfmBmYpefDeOvOtPNhQn/L2v8gXQK8p6quHS6vqnuq52HgLGb3udB/7t9B797bfen9vuzYfn/G3eaATMa/bb8NXFRVjwwXzNbzIcnLgfcAh/ddUZt2fxsMh+t2A7BHe2rsCfT+Ubt4xDYXA8NPFB0JfKXdE3Ax8Nr0ntx8BrAHvRtKx9LmdLLRY5BkR3p//E+pqiXDGyeZm2RBW94S+A3gZqa3iYzDTknmACR5Jr1z4Y6quge4N8kL22XUNwL/uikOZgIm8jtBki3o/QPQ3W84i8+H0XwJ+LUkT0zyRODXgC/NwPNho8egbX8R8OmqumDEuoXtZ+jdVzVrz4V2DsxrywuAlwBfa78vV9D7/YHe79N0Phdgcv5tex0j/sdxNp4PSfYF/oFeMPx+36rp97dhsp9wmS0v4JXA7fRme97Tyt7f/qMCbAV8jt4DJ9cDz+yr+55W7zb6nixaV5vT+bWxYwC8F3gAWN73egqwLbAMWEnvQZW/BeYM+jincBxe045zOXAj8Jt9bS6m98fuW8Df0T6Qfjq/Jvg7cQhw7Yj2Zuv5sD+9e4MeoDcTdEtf3d9p4/NNepdUZ+T5sLFjALwBeGTE34ZFbd1XgFVtHM4Fthv0cU7hOLy4HeuK9vP4vjaf2X5/vtl+n+YN+jinahzauiF6s2FbjGhzNp4P/w58r+/cv7iv7rT62+A3pEiSJKnjZWVJkiR1DIeSJEnqGA4lSZLUMRxKkiSpYziUJElSx3AoaVZL8lj7er7h19BGtHFEkj2noHskGUqyST/DLcmiJK/clPuUNHPM3fAmkjSjPVRViybYxhHAF4CvjbVCkrn1+HfkThvt2zcW0fv8tP9vsL2RNB05cyhps5NkvyRXJVmW5Et938bw5iQ3JFmR5PNJtknyYuBw4PQ287hbkiuTLG51FiRZ3ZaPS3Jxkq8AlyfZNsknk1yf5KYkr9pAv45L8i9JLkuyOsnbkpzc6l6b5EltuyuT/G3rz81JDmjlT2r1V7btn9fKT01yTpIl9L7T+P3AUa3+UUkOSHJN289/JnlWX38uTHJpkm8k+cu+vr4iyY1trC5vZeM6XknTkzOHkma7rZMsb8t30vsav48Br6qqHyQ5Cvhzet9QcGFV/R+AJB+k980VH0tyMfCFal/51vsmq1E9H3heVf04yYfofY3g76T3tZLXJ/n3qnpgPfX3ovc9u1vR+7aEP6mqfZN8hN7XZ53RttumqhYlORj4ZKv3Z8BNVXVEkpcBn6Y3SwiwJ3BgVT2U5DhgcVW9rR3PDsBBVfVoet/9+iF63/BDq78v8DBwW5KPAT8D/g9wcFXdORxa6X071HiPV9I0YziUNNv93GXlJHvRC1KXtZA3B7inrd6rhcIdge3ofefpeF1WVT9uy78GHJ7kne39VsDTgVvXU/+KqroPuC/JGuDfWvkq4Hl9250PUFVfTbJDC2MH0kJdVX0lyZNb8IPeV3U9NMo+5wOfSrIHUMCWfesur6o1AEm+BuwKPBH4alXd2fY1keOVNM0YDiVtbkLvu11ftI51ZwNHVNWKNrt2yChtPMrjt+VsNWJd/yxZgNdU1W3j6N/Dfctr+96v5ef/Zo/87tMNfRfq+mbvPkAvlL66PbBz5Sj9eYz1/7uxMccraZrxnkNJm5vbgJ2SvAggyZZJntvWbQ/ck2RL4Oi+Ove1dcNWA/u15SPXs68vAX+QNkWZZN+Jd79zVGvzQGBNm927mtbvJIcAP6yqe9dRd+TxzAfuasvHjWHf1wIHJ3lG29fwZeWpPF5Jm4jhUNJmpar+m16g+3CSFcBy4MVt9fuA64AlwNf7qn0G+KP2kMVuwF8BJya5CViwnt19gN4l2pVJbmnvJ8vP2v4/Dhzfyk4F9kuyEjgNOHaUulcAew4/kAL8JfAXrb0NXlGqqh8AJwAXtjH8bFs1lccraRNJ1YauREiSppMkVwLvrKqlg+6LpNnHmUNJkiR1nDmUJElSx5lDSZIkdQyHkiRJ6hgOJUmS1DEcSpIkqWM4lCRJUsdwKEmSpM7/D7avvzCWvT9MAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PLOT FEATURE IMPORTANCE FOR SINGLE SPECIES\n",
    "\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "feature_names = features.columns\n",
    "sorted_indices = feature_importances.argsort()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_indices)), feature_importances[sorted_indices], align='center')\n",
    "plt.yticks(range(len(sorted_indices)), feature_names[sorted_indices])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance Plot')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI OUTPUT CLASSIFIER.\n",
    "\n",
    "select a range of species here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Atl_croaker_(nibea98):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Bay_anchovy:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Black_drum_or_Spot:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.20      0.33         5\n",
      "\n",
      "    accuracy                           0.20         5\n",
      "   macro avg       0.50      0.10      0.17         5\n",
      "weighted avg       1.00      0.20      0.33         5\n",
      "\n",
      "Classification report for Black_sea_bass:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.25      0.33      0.29         5\n",
      "weighted avg       0.30      0.40      0.34         5\n",
      "\n",
      "Classification report for Brd_striped_anchovy:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Little_tunny_or_skipjack_tuna:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Nor_sea_robin:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Scup:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.83      0.83      0.80         5\n",
      "weighted avg       0.87      0.80      0.80         5\n",
      "\n",
      "Classification report for Smallmouth_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      0.25      0.33         4\n",
      "\n",
      "    accuracy                           0.20         5\n",
      "   macro avg       0.25      0.12      0.17         5\n",
      "weighted avg       0.40      0.20      0.27         5\n",
      "\n",
      "Classification report for Southern_kingfish(nibea95):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Str_sea_robin:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Summ_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.50      0.40      0.44         5\n",
      "weighted avg       1.00      0.80      0.89         5\n",
      "\n",
      "Classification report for Summ_flounder99a:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.58      0.58      0.58         5\n",
      "weighted avg       0.60      0.60      0.60         5\n",
      "\n",
      "Classification report for Weakfish_Cyn:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Windowpane_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Am_butterfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.50      0.40      0.44         5\n",
      "weighted avg       1.00      0.80      0.89         5\n",
      "\n",
      "Classification report for Atl_chub_mackerel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Frigate_or_bullet_tuna:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Giant_trevally99:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Hogchoker_trinectes:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Nor_kingfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57         2\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.20      0.50      0.29         5\n",
      "weighted avg       0.16      0.40      0.23         5\n",
      "\n",
      "Classification report for Red_White_or_Spotted_hake:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Spanish_mackerel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Tautog:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.58      0.58      0.58         5\n",
      "weighted avg       0.60      0.60      0.60         5\n",
      "\n",
      "Classification report for Thread_herring:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_menhaden_LS16_or_river_herrings:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_menhaden_LS17:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Cobia:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Cunner:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Am_conger:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Atl_or_nor_sand_lance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Bluefish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Silver_anchovy:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Seaboard_goby:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57         2\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.20      0.50      0.29         5\n",
      "weighted avg       0.16      0.40      0.23         5\n",
      "\n",
      "Classification report for Nor_puffer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Silver_perch(nibea93):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.37         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n",
      "Classification report for Sturgeon:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.37         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n",
      "Classification report for Crested_blenny_refseq_not_full_length:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Str_burrfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Dwarf_goatfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Fourspot_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Nor_sennet95:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Tuna_sp:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_moonfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_silverside:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Gulf_stream_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Str_cusk_eel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Nor_stargazer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Grey_triggerfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Nor_pipefish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Planehead_filefish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Rough_scad:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Str_killifish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Mummichog:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Rough_silverside94:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Sheepshead_minnow:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Str_bass:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.50      0.40      0.44         5\n",
      "weighted avg       1.00      0.80      0.89         5\n",
      "\n",
      "Classification report for White_mullet:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Am_eel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Red_eye_round_herring:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Inshore_lizardfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_mackerel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Flathead_grey_mullet:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57         2\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.20      0.50      0.29         5\n",
      "weighted avg       0.16      0.40      0.23         5\n",
      "\n",
      "Classification report for Silver_hake:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.30      0.37         5\n",
      "weighted avg       1.00      0.60      0.75         5\n",
      "\n",
      "Classification report for Winter_or_Yellowtail_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.37         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n",
      "Classification report for Atl_herring:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.33      0.25      0.29         5\n",
      "weighted avg       0.53      0.40      0.46         5\n",
      "\n",
      "Classification report for Atl_cod:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Am_gizzard_shad:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Fourspine_stickleback:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Catfish_sp:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for White_perch:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Whitefish_Cor:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Pac_sand_lance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for White_catfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_salmon:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## MULTI OUTPUT CLASSIFIER\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Select features (oceanographic variables) and labels (presence/absence of multiple species)\n",
    "features = ml_df2[['latitude', 'longitude', 'surface_temp', 'surface_salt', 'bottom_temp', 'bottom_salt', 'surface_pH', 'bottom_pH']]\n",
    "labels = ml_df2.loc[:, 'Atl_croaker_(nibea98)':'Atl_salmon'].applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the multi-output Random Forest classifier\n",
    "rf_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "for i, species in enumerate(labels.columns):\n",
    "    accuracy = accuracy_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    # precision = precision_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    # recall = recall_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    # f1 = f1_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    # precisions.append(precision)\n",
    "    # recalls.append(recall)\n",
    "    # f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Classification report for {species}:\\n\", classification_report(y_test.iloc[:, i], y_pred[:, i]))\n",
    "    # print(f\"Accuracy for {species}: {accuracy}\")\n",
    "\n",
    "# Calculate overall average accuracy across all species\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_accuracy\n",
    "\n",
    "# # Evaluate the model\n",
    "# for i, species in enumerate(labels.columns):\n",
    "#     print(f\"Classification report for {species}:\\n\", classification_report(y_test.iloc[:, i], y_pred[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atl_croaker' 'Bay_anchovy']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "labels = ['Atl_croaker', 'Bay_anchovy']\n",
    "label_encoder.fit(labels)\n",
    "print(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame() # type: ignore\n",
    "for i, species in enumerate(labels.columns):\n",
    "    importances = rf_classifier.estimators_[i].feature_importances_\n",
    "    feature_importance_df[species] = importances\n",
    "\n",
    "# Calculate mean feature importance across all species\n",
    "feature_importance_df['mean_importance'] = feature_importance_df.mean(axis=1)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(features.columns, feature_importance_df['mean_importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Mean Feature Importance Across All Species')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "## Figure out units and interpretation. Test different hyperparams to maximize interpretability + accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEXT STEP: LOOK AT CORRELATIONS BETWEEN MULTIPLE SPECIES PRESENT WITHIN A CLUSTER?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
