{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIES CLASSIFIER MODEL RUN WITHOUT LAT LONG AND PH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Atl_croaker_(nibea98)</th>\n",
       "      <th>Bay_anchovy</th>\n",
       "      <th>Black_drum_or_Spot</th>\n",
       "      <th>Black_sea_bass</th>\n",
       "      <th>Brd_striped_anchovy</th>\n",
       "      <th>Little_tunny_or_skipjack_tuna</th>\n",
       "      <th>Nor_sea_robin</th>\n",
       "      <th>Scup</th>\n",
       "      <th>Smallmouth_flounder</th>\n",
       "      <th>Southern_kingfish(nibea95)</th>\n",
       "      <th>...</th>\n",
       "      <th>Catfish_sp</th>\n",
       "      <th>White_perch</th>\n",
       "      <th>Whitefish_Cor</th>\n",
       "      <th>Pac_sand_lance</th>\n",
       "      <th>White_catfish</th>\n",
       "      <th>Atl_salmon</th>\n",
       "      <th>surface_temp</th>\n",
       "      <th>surface_salt</th>\n",
       "      <th>bottom_temp</th>\n",
       "      <th>bottom_salt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240067</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.207000</td>\n",
       "      <td>19.924286</td>\n",
       "      <td>6.984000</td>\n",
       "      <td>22.357692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010920</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.572200</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>7.327800</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.175250</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>7.034833</td>\n",
       "      <td>22.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>0.017985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.141714</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>6.951571</td>\n",
       "      <td>0.066429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044081</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.747545</td>\n",
       "      <td>21.470000</td>\n",
       "      <td>7.162100</td>\n",
       "      <td>22.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013282</td>\n",
       "      <td>0.025096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015334</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.412000</td>\n",
       "      <td>22.007000</td>\n",
       "      <td>7.383020</td>\n",
       "      <td>22.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.550700</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>5.717857</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015507</td>\n",
       "      <td>0.038603</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036010</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.890875</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.516107</td>\n",
       "      <td>20.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.051945</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.065100</td>\n",
       "      <td>18.135000</td>\n",
       "      <td>6.225100</td>\n",
       "      <td>21.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008507</td>\n",
       "      <td>0.069150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.678375</td>\n",
       "      <td>22.031250</td>\n",
       "      <td>6.654333</td>\n",
       "      <td>22.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.123857</td>\n",
       "      <td>22.711429</td>\n",
       "      <td>7.379176</td>\n",
       "      <td>15.897059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.018294</td>\n",
       "      <td>0.022694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.037855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.699333</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>7.938500</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.017572</td>\n",
       "      <td>0.024368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.214500</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>7.776800</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.955429</td>\n",
       "      <td>20.768571</td>\n",
       "      <td>7.200857</td>\n",
       "      <td>20.779286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031501</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.321667</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>7.589667</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.898286</td>\n",
       "      <td>21.882857</td>\n",
       "      <td>7.142786</td>\n",
       "      <td>22.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.012054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010711</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.343000</td>\n",
       "      <td>11.487500</td>\n",
       "      <td>7.461889</td>\n",
       "      <td>22.536667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.013465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.542500</td>\n",
       "      <td>19.558750</td>\n",
       "      <td>6.364842</td>\n",
       "      <td>21.901579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.032000</td>\n",
       "      <td>21.691667</td>\n",
       "      <td>6.526176</td>\n",
       "      <td>22.053529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.131667</td>\n",
       "      <td>17.976667</td>\n",
       "      <td>6.400722</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.693222</td>\n",
       "      <td>21.846667</td>\n",
       "      <td>6.477400</td>\n",
       "      <td>22.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Atl_croaker_(nibea98)  Bay_anchovy  Black_drum_or_Spot  Black_sea_bass  \\\n",
       "0                0.000000     0.240067            0.008431        0.000000   \n",
       "1                0.000000     0.010920            0.000726        0.000065   \n",
       "2                0.000000     0.000000            0.000000        0.000000   \n",
       "3                0.000000     0.075304            0.000000        0.000000   \n",
       "4                0.000000     0.158802            0.000000        0.000000   \n",
       "5                0.000000     0.013282            0.025096        0.000000   \n",
       "6                0.000000     0.008921            0.006158        0.000000   \n",
       "7                0.015507     0.038603            0.004211        0.000465   \n",
       "8                0.006269     0.051945            0.006176        0.000364   \n",
       "9                0.008507     0.069150            0.000000        0.000000   \n",
       "10               0.000000     0.000000            0.011916        0.000000   \n",
       "11               0.018294     0.022694            0.000000        0.010783   \n",
       "12               0.000000     0.021126            0.000000        0.000000   \n",
       "13               0.000000     0.000000            0.000000        0.000000   \n",
       "14               0.000000     0.031501            0.012988        0.003752   \n",
       "15               0.000000     0.009734            0.003974        0.000000   \n",
       "16               0.000000     0.024823            0.012054        0.000000   \n",
       "17               0.000000     0.013080            0.013465        0.000000   \n",
       "18               0.000000     0.189419            0.000000        0.000052   \n",
       "19               0.000000     0.000000            0.009373        0.000000   \n",
       "20               0.000000     0.011171            0.000000        0.001862   \n",
       "\n",
       "    Brd_striped_anchovy  Little_tunny_or_skipjack_tuna  Nor_sea_robin  \\\n",
       "0              0.000000                            0.0       0.005233   \n",
       "1              0.000000                            0.0       0.000610   \n",
       "2              0.000000                            0.0       0.028369   \n",
       "3              0.000000                            0.0       0.000000   \n",
       "4              0.000000                            0.0       0.044081   \n",
       "5              0.000000                            0.0       0.008265   \n",
       "6              0.000000                            0.0       0.018742   \n",
       "7              0.000000                            0.0       0.001601   \n",
       "8              0.000000                            0.0       0.000168   \n",
       "9              0.000000                            0.0       0.000000   \n",
       "10             0.000000                            0.0       0.000000   \n",
       "11             0.000000                            0.0       0.000000   \n",
       "12             0.000000                            0.0       0.008899   \n",
       "13             0.000000                            0.0       0.000000   \n",
       "14             0.000000                            0.0       0.000783   \n",
       "15             0.006435                            0.0       0.003774   \n",
       "16             0.000000                            0.0       0.006187   \n",
       "17             0.000000                            0.0       0.003142   \n",
       "18             0.000000                            0.0       0.000083   \n",
       "19             0.000000                            0.0       0.000000   \n",
       "20             0.000000                            0.0       0.000416   \n",
       "\n",
       "        Scup  Smallmouth_flounder  Southern_kingfish(nibea95)  ...  \\\n",
       "0   0.002279             0.006085                    0.003435  ...   \n",
       "1   0.000182             0.031328                    0.000000  ...   \n",
       "2   0.000000             0.000000                    0.000000  ...   \n",
       "3   0.016420             0.017985                    0.000000  ...   \n",
       "4   0.014342             0.000000                    0.000000  ...   \n",
       "5   0.000000             0.015334                    0.004269  ...   \n",
       "6   0.000000             0.036051                    0.000000  ...   \n",
       "7   0.000000             0.036010                    0.000402  ...   \n",
       "8   0.000000             0.033119                    0.000000  ...   \n",
       "9   0.000000             0.047961                    0.000000  ...   \n",
       "10  0.000000             0.141404                    0.000000  ...   \n",
       "11  0.016612             0.037855                    0.000000  ...   \n",
       "12  0.017572             0.024368                    0.000000  ...   \n",
       "13  0.000000             0.016927                    0.007816  ...   \n",
       "14  0.000000             0.008803                    0.008411  ...   \n",
       "15  0.000000             0.000000                    0.000000  ...   \n",
       "16  0.000000             0.010711                    0.009848  ...   \n",
       "17  0.001346             0.005450                    0.000000  ...   \n",
       "18  0.000000             0.000000                    0.000000  ...   \n",
       "19  0.007629             0.000000                    0.000000  ...   \n",
       "20  0.000000             0.000000                    0.000000  ...   \n",
       "\n",
       "    Catfish_sp  White_perch  Whitefish_Cor  Pac_sand_lance  White_catfish  \\\n",
       "0          0.0          0.0            0.0             0.0            0.0   \n",
       "1          0.0          0.0            0.0             0.0            0.0   \n",
       "2          0.0          0.0            0.0             0.0            0.0   \n",
       "3          0.0          0.0            0.0             0.0            0.0   \n",
       "4          0.0          0.0            0.0             0.0            0.0   \n",
       "5          0.0          0.0            0.0             0.0            0.0   \n",
       "6          0.0          0.0            0.0             0.0            0.0   \n",
       "7          0.0          0.0            0.0             0.0            0.0   \n",
       "8          0.0          0.0            0.0             0.0            0.0   \n",
       "9          0.0          0.0            0.0             0.0            0.0   \n",
       "10         0.0          0.0            0.0             0.0            0.0   \n",
       "11         0.0          0.0            0.0             0.0            0.0   \n",
       "12         0.0          0.0            0.0             0.0            0.0   \n",
       "13         0.0          0.0            0.0             0.0            0.0   \n",
       "14         0.0          0.0            0.0             0.0            0.0   \n",
       "15         0.0          0.0            0.0             0.0            0.0   \n",
       "16         0.0          0.0            0.0             0.0            0.0   \n",
       "17         0.0          0.0            0.0             0.0            0.0   \n",
       "18         0.0          0.0            0.0             0.0            0.0   \n",
       "19         0.0          0.0            0.0             0.0            0.0   \n",
       "20         0.0          0.0            0.0             0.0            0.0   \n",
       "\n",
       "    Atl_salmon  surface_temp  surface_salt  bottom_temp  bottom_salt  \n",
       "0          0.0      7.207000     19.924286     6.984000    22.357692  \n",
       "1          0.0      7.572200      0.060000     7.327800     0.037500  \n",
       "2          0.0      7.175250     18.300000     7.034833    22.330000  \n",
       "3          0.0      7.141714      0.008571     6.951571     0.066429  \n",
       "4          0.0      7.747545     21.470000     7.162100    22.380000  \n",
       "5          0.0      7.412000     22.007000     7.383020    22.400000  \n",
       "6          0.0      5.550700      0.083000     5.717857     0.105000  \n",
       "7          0.0      4.890875     19.000000     5.516107    20.027500  \n",
       "8          0.0      6.065100     18.135000     6.225100    21.723000  \n",
       "9          0.0      6.678375     22.031250     6.654333    22.150000  \n",
       "10         0.0      7.123857     22.711429     7.379176    15.897059  \n",
       "11         0.0      7.699333      0.051667     7.938500     0.028000  \n",
       "12         0.0      7.214500      0.028333     7.776800     0.041000  \n",
       "13         0.0      7.955429     20.768571     7.200857    20.779286  \n",
       "14         0.0      7.321667      0.038333     7.589667     0.087500  \n",
       "15         0.0      6.898286     21.882857     7.142786    22.370000  \n",
       "16         0.0      6.343000     11.487500     7.461889    22.536667  \n",
       "17         0.0      6.542500     19.558750     6.364842    21.901579  \n",
       "18         0.0      7.032000     21.691667     6.526176    22.053529  \n",
       "19         0.0      7.131667     17.976667     6.400722    22.000000  \n",
       "20         0.0      6.693222     21.846667     6.477400    22.030000  \n",
       "\n",
       "[21 rows x 79 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('/Users/henrysun_1/Desktop/Duke/2024-2025/Summer 2024/fishics/data/ctd/2024-02_data_all_exo.csv')\n",
    "df = df[df['station'] < 32] ## stations included in eDNA survey\n",
    "\n",
    "data = pd.read_csv(\"/Users/henrysun_1/Desktop/Duke/2024-2025/Summer 2024/fishics/data/data_for_henry_2024.csv\")\n",
    "data = data[data['date'] == '2/1/24']\n",
    "\n",
    "df = df.rename(columns={\"GPS.Latitude..\": \"lat\", \"GPS.Longitude..\": \"long\", \"Depth.m\":\"depth\",\"Sal.psu\":\"salt\",\"Temp..C\":\"temp\"})\n",
    "df = df[['lat','long','depth','salt','station','temp','pH']]\n",
    "\n",
    "# Group by station\n",
    "grouped = df.groupby('station')\n",
    "\n",
    "# Initialize lists to hold the aggregated data\n",
    "stations = []\n",
    "surface_temps = []\n",
    "surface_salts = []\n",
    "surface_pHs = []\n",
    "bottom_temps = []\n",
    "bottom_salts = []\n",
    "bottom_pHs = []\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "\n",
    "# Iterate over each group\n",
    "for station, group in grouped:\n",
    "    # Find the row with the minimum depth (surface)\n",
    "    min_depth_row = group.loc[group['depth'].idxmin()]\n",
    "    # Find the row with the maximum depth (bottom)\n",
    "    max_depth_row = group.loc[group['depth'].idxmax()]\n",
    "    \n",
    "    # Append the results to the lists. Took out lat/long/pH for this\n",
    "    stations.append(station)\n",
    "    surface_temps.append(min_depth_row['temp'])\n",
    "    surface_salts.append(min_depth_row['salt'])\n",
    "    # surface_pHs.append(min_depth_row['pH'])\n",
    "    bottom_temps.append(max_depth_row['temp'])\n",
    "    bottom_salts.append(max_depth_row['salt'])\n",
    "    # bottom_pHs.append(max_depth_row['pH'])\n",
    "    # latitudes.append(min_depth_row['lat'])  # assuming latitude is constant for each station\n",
    "    # longitudes.append(min_depth_row['long'])  # assuming longitude is constant for each station\n",
    "\n",
    "aggregated_df = pd.DataFrame({\n",
    "    'station': stations,\n",
    "    # 'latitude': latitudes,\n",
    "    # 'longitude': longitudes,\n",
    "    'surface_temp': surface_temps,\n",
    "    'surface_salt': surface_salts,\n",
    "    # 'surface_pH': surface_pHs,\n",
    "    'bottom_temp': bottom_temps,\n",
    "    'bottom_salt': bottom_salts,\n",
    "    # 'bottom_pH': bottom_pHs\n",
    "})\n",
    "\n",
    "# Perform an inner join on the 'station' column\n",
    "merged_df = data.merge(aggregated_df, on='station', how='inner')\n",
    "fish_asvs = merged_df.iloc[:,11:86]\n",
    "merged_df\n",
    "ml_df2 = merged_df.iloc[:,11:] # type: ignore\n",
    "# ml_df2 = ml_df2.drop(columns = ['tSNE1', 'tSNE2'])\n",
    "ml_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Atl_croaker_(nibea98):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Bay_anchovy:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Black_drum_or_Spot:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.50      0.20      0.29         5\n",
      "weighted avg       1.00      0.40      0.57         5\n",
      "\n",
      "Classification report for Black_sea_bass:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.25      0.33      0.29         5\n",
      "weighted avg       0.30      0.40      0.34         5\n",
      "\n",
      "Classification report for Brd_striped_anchovy:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Little_tunny_or_skipjack_tuna:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Nor_sea_robin:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.50      0.40      0.44         5\n",
      "weighted avg       1.00      0.80      0.89         5\n",
      "\n",
      "Classification report for Scup:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         2\n",
      "           1       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.75      0.67      0.58         5\n",
      "weighted avg       0.80      0.60      0.57         5\n",
      "\n",
      "Classification report for Smallmouth_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40         1\n",
      "           1       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.62      0.62      0.40         5\n",
      "weighted avg       0.85      0.40      0.40         5\n",
      "\n",
      "Classification report for Southern_kingfish(nibea95):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Str_sea_robin:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Summ_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.50      0.40      0.44         5\n",
      "weighted avg       1.00      0.80      0.89         5\n",
      "\n",
      "Classification report for Summ_flounder99a:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.58      0.58      0.58         5\n",
      "weighted avg       0.60      0.60      0.60         5\n",
      "\n",
      "Classification report for Weakfish_Cyn:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Windowpane_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Am_butterfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.50      0.40      0.44         5\n",
      "weighted avg       1.00      0.80      0.89         5\n",
      "\n",
      "Classification report for Atl_chub_mackerel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Frigate_or_bullet_tuna:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Giant_trevally99:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Hogchoker_trinectes:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Nor_kingfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         2\n",
      "           1       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.75      0.67      0.58         5\n",
      "weighted avg       0.80      0.60      0.57         5\n",
      "\n",
      "Classification report for Red_White_or_Spotted_hake:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Spanish_mackerel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Tautog:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.58      0.58      0.58         5\n",
      "weighted avg       0.60      0.60      0.60         5\n",
      "\n",
      "Classification report for Thread_herring:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_menhaden_LS16_or_river_herrings:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_menhaden_LS17:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Cobia:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Cunner:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Am_conger:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Atl_or_nor_sand_lance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.33      0.25      0.29         5\n",
      "weighted avg       0.53      0.40      0.46         5\n",
      "\n",
      "Classification report for Bluefish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Silver_anchovy:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Seaboard_goby:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.83      0.83      0.80         5\n",
      "weighted avg       0.87      0.80      0.80         5\n",
      "\n",
      "Classification report for Nor_puffer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Silver_perch(nibea93):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.37         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n",
      "Classification report for Sturgeon:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.37         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n",
      "Classification report for Crested_blenny_refseq_not_full_length:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Str_burrfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Dwarf_goatfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Fourspot_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Nor_sennet95:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Tuna_sp:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_moonfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_silverside:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Gulf_stream_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Str_cusk_eel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Nor_stargazer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Grey_triggerfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Nor_pipefish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.83      0.83      0.80         5\n",
      "weighted avg       0.87      0.80      0.80         5\n",
      "\n",
      "Classification report for Planehead_filefish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Rough_scad:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Str_killifish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Mummichog:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Rough_silverside94:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Sheepshead_minnow:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Str_bass:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.30      0.37         5\n",
      "weighted avg       1.00      0.60      0.75         5\n",
      "\n",
      "Classification report for White_mullet:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Am_eel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Red_eye_round_herring:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Inshore_lizardfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_mackerel:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Flathead_grey_mullet:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57         2\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.20      0.50      0.29         5\n",
      "weighted avg       0.16      0.40      0.23         5\n",
      "\n",
      "Classification report for Silver_hake:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.30      0.37         5\n",
      "weighted avg       1.00      0.60      0.75         5\n",
      "\n",
      "Classification report for Winter_or_Yellowtail_flounder:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.25      0.33      0.29         5\n",
      "weighted avg       0.30      0.40      0.34         5\n",
      "\n",
      "Classification report for Atl_herring:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.33      0.25      0.29         5\n",
      "weighted avg       0.53      0.40      0.46         5\n",
      "\n",
      "Classification report for Atl_cod:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Classification report for Am_gizzard_shad:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Fourspine_stickleback:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Catfish_sp:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for White_perch:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Whitefish_Cor:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Pac_sand_lance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for White_catfish:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Classification report for Atl_salmon:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8613333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "## MULTI OUTPUT CLASSIFIER\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Select features (oceanographic variables) and labels (presence/absence of multiple species)\n",
    "features = ml_df2[['surface_temp', 'surface_salt', 'bottom_temp', 'bottom_salt']]\n",
    "labels = ml_df2.loc[:, 'Atl_croaker_(nibea98)':'Atl_salmon'].applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the multi-output Random Forest classifier\n",
    "rf_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "for i, species in enumerate(labels.columns):\n",
    "    accuracy = accuracy_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    # precision = precision_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    # recall = recall_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    # f1 = f1_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    # precisions.append(precision)\n",
    "    # recalls.append(recall)\n",
    "    # f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Classification report for {species}:\\n\", classification_report(y_test.iloc[:, i], y_pred[:, i]))\n",
    "    # print(f\"Accuracy for {species}: {accuracy}\")\n",
    "\n",
    "# Calculate overall average accuracy across all species\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_accuracy\n",
    "\n",
    "# # Evaluate the model\n",
    "# for i, species in enumerate(labels.columns):\n",
    "#     print(f\"Classification report for {species}:\\n\", classification_report(y_test.iloc[:, i], y_pred[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "feature_importance_df = pd.DataFrame() # type: ignore\n",
    "for i, species in enumerate(labels.columns):\n",
    "    importances = rf_classifier.estimators_[i].feature_importances_\n",
    "    feature_importance_df[species] = importances\n",
    "\n",
    "# Calculate mean feature importance across all species\n",
    "feature_importance_df['mean_importance'] = feature_importance_df.mean(axis=1)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(features.columns, feature_importance_df['mean_importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Mean Feature Importance Across All Species')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "## Figure out units and interpretation. Test different hyperparams to maximize interpretability + accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
